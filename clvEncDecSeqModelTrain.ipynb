{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clvEncDecSeqModelTrain.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMude/LDwVU4RuS7stswRcp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corrine1106/clv_pred/blob/main/clvEncDecSeqModelTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyreadr"
      ],
      "metadata": {
        "id": "5S1LpEeeYMD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine.input_layer import Input\n",
        "from tf.keras.layers import Lambda, Embedding, Reshape, Conv1D, Concatenate, GRU, Dense, TimeDistributed\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "ctGlqZkjPtRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os.path\n",
        "import pyreadr\n",
        "import json "
      ],
      "metadata": {
        "id": "SRYWJp4djgIS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "031bb66a-43e1-48fe-ea28-478dfaa4a2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c5acf40942c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyreadr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyreadr'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScmC4Ek5dxI0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "collapsed": true,
        "outputId": "f9eb2c96-6a66-409b-e60e-19b1621cea44"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-6e89b33d51ba>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    if (length(embeddingFeatVec) > 0) and (!useSkipGramEmbeddings):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "allSeqFeatLayerLength = seqInLength\n",
        "\n",
        "# 用list comprehension轉換match function in R\n",
        "seqFeatIndVec = [ embeddingFeatVec.index(i) for i in seqFeatVec]\n",
        "\n",
        "if (length(embeddingFeatVec) > 0) and (!useSkipGramEmbeddings):\n",
        "    # ref:https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\n",
        "    # data.table會把不足的值重複，但是zip不會 不知道會不會影響\n",
        "    embeddingConfigDT = pd.DataFrame(list(zip(embeddingFeatVec,seqFeatIndVec,\n",
        "                                              apply(lambda ind: max(X_DT[[embeddingFeatVec[ind]]]) + 1,range(len(embeddingFeatVec))),\n",
        "                                              embedDimVec,[allSeqFeatLayerLength]*len(embeddingFeatVec))),\n",
        "               columns =['seq_feat', 'seq_feat_ind','input_dim','output_dim','input_length'])\n",
        "else:\n",
        "    # R 可以創一個空的df但是pandas好像不行，所以我先創一個跟上面一樣columns的df\n",
        "    embeddingConfigDT = pd.Dataframe(columns =['seq_feat', 'seq_feat_ind','input_dim','output_dim','input_length'])\n",
        "\n",
        "if useSkipGramEmbeddings:\n",
        "    # paste0把兩個接起來\n",
        "    # example:\n",
        "    # nth <- paste0(1:12, c(\"st\", \"nd\", \"rd\", rep(\"th\", 9)))\n",
        "    #  [1] \"1st\"  \"2nd\"  \"3rd\"  \"4th\"  \"5th\"  \"6th\"  \"7th\"  \"8th\"  \"9th\"  \"10th\" \"11th\"\n",
        "    # [12] \"12th\"\n",
        "    if os.path.isfile(path+skipGramEmbeddingFilename):\n",
        "        # readRDS是讀取讀取rsd檔案（R物件）\n",
        "        # ref:https://stackoverflow.com/questions/40996175/loading-a-rds-file-in-pandas\n",
        "        skipGramEmbeddingMatrix = pyreadr.read_r(path+skipGramEmbeddingFilename)\n",
        "    # shape because it's column\n",
        "    embedDimVec = [skipGramEmbeddingMatrix.shape[1]]*len(embeddingFeatVec)\n",
        "    seqFeatIndVec = [ embeddingFeatVec.index(i) for i in seqFeatVec]\n",
        "    skipGramNumWords = skipGramEmbeddingMatrix.shape[0]\n",
        "    embeddingConfigDT = pd.DataFrame(list(zip(embeddingFeatVec,seqFeatIndVec,\n",
        "                                              skipGramNumWords,\n",
        "                                              embedDimVec,[allSeqFeatLayerLength]*len(embeddingFeatVec))),\n",
        "               columns =['seq_feat', 'seq_feat_ind','input_dim','output_dim','input_length'])\n",
        "# 原match function: match(convFeatVec, seqFeatVec)\n",
        "seqFeatIndVec = [ convFeatVec.index(i) for i in seqFeatVec]\n",
        "\n",
        "\n",
        "if len(convFeatVec) > 0:\n",
        "    convConfigDT = pd.DataFrame(list(zip(convFeatVec,seqFeatIndVec,\n",
        "                                              filterVec,\n",
        "                                              kernelSizeVec,[convPaddingType]*len(convFeatVec))),\n",
        "               columns =['seq_feat', 'seq_feat_ind','filters','kernel_size','padding'])\n",
        "\n",
        "else:\n",
        "    convConfigDT = pd.Dataframe(columns =['seq_feat', 'seq_feat_ind','filters','kernel_size','padding'])\n",
        "\n",
        "\n",
        "if len(convFeatVec) > 0:\n",
        "    convConfigLayer2DT = pd.DataFrame(list(zip(convFeatVec,seqFeatIndVec,\n",
        "                                              filterLayer2Vec,\n",
        "                                              kernelSizeLayer2Vec,[convPaddingType]*len(convFeatVec))),\n",
        "               columns =['seq_feat', 'seq_feat_ind','filters','kernel_size','padding'])\n",
        "\n",
        "else:\n",
        "    convConfig2DT = pd.Dataframe(columns =['seq_feat', 'seq_feat_ind','filters','kernel_size','padding'])\n",
        "\n",
        "# layer_input應該是input layer的意思 指定input的規格\n",
        "allSeqFeatLayerInputs = Input(shape = (allSeqFeatLayerLength, numSeqFeat), name = 'allSeqFeatLayerInputs')\n",
        "# > rep(\"\", 3)\n",
        "# [1] \"\" \"\" \"\"\n",
        "# 這個形式沒有直接對應的\n",
        "# https://www.r-bloggers.com/2014/02/character-strings-in-r/\n",
        "# 應該可以當作list of string 看待\n",
        "sliceSeqFeatNameVec = [\"\"]*numSeqFeat\n",
        "\n",
        "sliceSeqFeatLayerList = [np.NaN]\n",
        "\n",
        "for sliceInd in range(numSeqFeat):\n",
        "    sliceSeqFeatNameVec[sliceInd] = \"sliceSeqFeat_\" + seqFeatVec[sliceInd]\n",
        "    sliceSeqFeatLayerList[sliceInd] = Lambda(function = lambda x: x[, , sliceInd], name = \"seqFeatVec[%s]\"%(sliceInd)))(allSeqFeatLayerInputs)\n",
        "\n",
        "sliceSeqFeatLayerInputList = sliceSeqFeatLayerList\n",
        "\n",
        "sliceEmbedFeatNameVec = [\"\"]*embeddingConfigDT.shape[0]\n",
        "sliceEmbedFeatLayerList = [np.NaN]\n",
        "\n",
        "if len(embeddingConfigDT) > 0:\n",
        "    for embFeatInd in range(len(embeddingConfigDT)) :\n",
        "\n",
        "        sliceEmbedFeatNameVec[embFeatInd] = \"sliceEmbedFeat_\" + embeddingConfigDT.loc[embFeatInd,\"seq_feat\"]\n",
        "        sliceEmbedFeatNameVec[embFeatInd] = Embedding(  input_dim       =   embeddingConfigDT.loc[embFeatInd,'input_dim'],\n",
        "                                                        output_dim      =   embeddingConfigDT.loc[embFeatInd,'output_dim'],\n",
        "                                                        input_length    =   embeddingConfigDT.loc[embFeatInd,'input_length'],\n",
        "                                                        name            =   embeddingConfigDT.loc[embFeatInd,'seq_feat']+'_embed')(sliceSeqFeatLayerList[embeddingConfigDT.loc[embFeatInd,\"seq_feat_ind\"]])\n",
        "        sliceEmbedFeatLayerList[embeddingConfigDT.loc[embFeatInd,'seq_feat_ind']] = sliceEmbedFeatNameVec[embFeatInd]\n",
        "        sliceSeqFeatLayerList[embeddingConfigDT.loc[embFeatInd,'seq_feat_ind']] = sliceEmbedFeatNameVec[embFeatInd]\n",
        "\n",
        "# > setdiff(1:6, 2:4)\n",
        "# [1] 1 5 6\n",
        "# ref:https://stackoverflow.com/questions/3462143/get-difference-between-two-lists\n",
        "for reshapeInd in list(set(range(len(seqFeatVec)))-set(embeddingConfigDT['seq_feat_ind'])):\n",
        "    sliceSeqFeatLayerList[reshapeInd] =  Reshape(target_shape = (dim(sliceSeqFeatLayerList[reshapeInd])[2], 1), \n",
        "                                                                            name = seqFeatVec[reshapeInd]+'_reshape')(sliceSeqFeatLayerList[reshapeInd])\n",
        "\n",
        "sliceConvFeatNameVec = [\"\"]*len(convConfigDT)\n",
        "\n",
        "if len(convConfigDT) > 0:\n",
        "    for convFeatInd in range(len(convConfigDT)):\n",
        "        sliceConvFeatNameVec[convFeatInd] = \"sliceConvFeat_\"+convConfigDT.loc[convFeatInd,'seq_feat']\n",
        "        sliceConvFeatNameVec[convFeatInd] = Conv1D(  filters     =   convConfigDT.loc[convFeatInd,'filters'],\n",
        "                                                     kernel_size =   convConfigDT.loc[convFeatInd,'kernel_size'],\n",
        "                                                     padding     =   convConfigDT.loc[convFeatInd,'padding'],\n",
        "                                                     name        =   convConfigDT.loc[convFeatInd,'seq_feat'] + '_conv'))(sliceSeqFeatLayerList[convConfigDT.loc[convFeatInd,seq_feat_ind]])\n",
        "        \n",
        "    if !useSeparateConv:\n",
        "        sliceSeqFeatLayerList[convConfigDT.loc[convFeatInd,'seq_feat_ind']] = sliceConvFeatNameVec[convFeatInd]\n",
        "\n",
        "sliceConvLayer2FeatNameVec = [\"\"]*len(convConfigLayer2DT)\n",
        "\n",
        "if len(convConfigLayer2DT) > 0:\n",
        "    for convFeatInd in range(len(convConfigLayer2DT)):\n",
        "        sliceConvLayer2FeatNameVec[convFeatInd] = \"sliceConvLayerFeat_\"+convConfigLayer2DT.loc[convFeatInd,'seq_feat']\n",
        "        if (!useSeparateConv):\n",
        "            sliceConvLayer2FeatNameVec[convFeatInd] = Conv1D( filters     =   convConfigLayer2DT.loc[convFeatInd,'filters'],\n",
        "                                                              kernel_size =   convConfigLayer2DT.loc[convFeatInd,'kernel_size'],\n",
        "                                                              padding     =   convConfigLayer2DT.loc[convFeatInd,'padding'],\n",
        "                                                              name        =   convConfigLayer2DT.loc[convFeatInd,'seq_feat'] + '_conv_layer2'))(sliceSeqFeatLayerList[convConfigLayer2DT.loc[convFeatInd,seq_feat_ind]])\n",
        "                \n",
        "            sliceSeqFeatLayerList[convConfigLayer2DT.loc[convFeatInd,'seq_feat_ind']] = sliceConvLayer2FeatNameVec[convFeatInd]\n",
        "\n",
        "        else:\n",
        "            sliceConvLayer2FeatNameVec[convFeatInd] = Conv1D( filters     =   convConfigLayer2DT.loc[convFeatInd,'filters'],\n",
        "                                                              kernel_size =   convConfigLayer2DT.loc[convFeatInd,'kernel_size'],\n",
        "                                                              padding     =   convConfigLayer2DT.loc[convFeatInd,'padding'],\n",
        "                                                              name        =   convConfigLayer2DT.loc[convFeatInd,'seq_feat'] + '_conv_layer2'))(sliceConvFeatNameVec[convFeatInd])\n",
        "\n",
        "\n",
        "if len(seqFeatVec) > 1:\n",
        "    if (!useSeparateConv):\n",
        "        encoderSeqFeat = Concatenate(axis = 2)(sliceSeqFeatLayerList)\n",
        "    else:\n",
        "        sliceSeqFeatLayerAddConvList = sliceSeqFeatLayerList\n",
        "        currLength = len(sliceSeqFeatLayerAddConvList)\n",
        "\n",
        "    if len(convConfigDT) > 0:\n",
        "        for convFeatInd in range(len(convConfigDT)):\n",
        "            sliceSeqFeatLayerAddConvList[currLength + convFeatInd] = sliceConvFeatNameVec[convFeatInd]\n",
        "\n",
        "    currLength = len(sliceSeqFeatLayerAddConvList)\n",
        "\n",
        "    if len(convConfigLayer2DT) > 0:\n",
        "        for convFeatInd in range(len(convConfigLayer2DT)):\n",
        "            sliceSeqFeatLayerAddConvList[currLength + convFeatInd] = sliceConvLayer2FeatNameVec[convFeatInd]\n",
        "\n",
        "    encoderSeqFeat = Concatenate(axis = 2)(sliceSeqFeatLayerAddConvList)\n",
        "\n",
        "else:\n",
        "    encoderSeqFeat = sliceSeqFeatLayerList[1]\n",
        "\n",
        "encoderGRULayer = GRU(units = nUnitsEncoder, dropout = dropoutEncoder, recurrent_dropout = recurrentDropoutEncoder, \n",
        "                                        activation = activationGRU, kernel_initializer = kernelInitGRU, return_state = TRUE)\n",
        "\n",
        "encoderOutputList =  encoderGRULayer(encoderSeqFeat)\n",
        "hiddenStatesEncoderGRU = encoderOutputList[2]\n",
        "\n",
        "\n",
        "if (nUnitsEncoder_2 > 0):\n",
        "    nUnitsEncoder_2 = nUnitsEncoder\n",
        "    hiddenStatesEncoderGRU_1 = hiddenStatesEncoderGRU\n",
        "    if (useDenseEncoder1ToEncoder2Layer):\n",
        "        hiddenStatesEncoderGRU_1 = Dense(units = nUnitsEncoder_2, activation = \"tanh\")(hiddenStatesEncoderGRU_1)\n",
        "\n",
        "    encoder_2_outputList = GRU(units = nUnitsEncoder_2, dropout = dropoutEncoder, recurrent_dropout = recurrentDropoutEncoder, \n",
        "                                            activation = activationGRU, kernel_initializer = kernelInitGRU,\n",
        "                                                return_state = TRUE)(initial_state = hiddenStatesEncoderGRU_1)(encoderSeqFeat)\n",
        "    hiddenStatesEncoderGRU_2 = encoder_2_outputList[[2]]\n",
        "    hiddenStatesEncoderGRU = hiddenStatesEncoderGRU_2\n",
        "\n",
        "if (useDenseEncoderToDecoderLayer):\n",
        "    hiddenStatesEncoderGRU = Dense(units = nUnitsDecoder, activation = \"tanh\")(hiddenStatesEncoderGRU)\n",
        "\n",
        "decoderSeqFeat = encoderSeqFeat\n",
        "\n",
        "decoderSliceOutput = Lambda(function = lambda xVal: xVal[, (seqInLength - seqOutLength + 1):seqInLength, ])\n",
        "\n",
        "decoderOutputs = decoderSliceOutput(TimeDistributed(layer = Dense(units = 1))(GRU(units = nUnitsDecoder, dropout = dropoutDecoder, recurrent_dropout = recurrentDropoutDecoder, \n",
        "                                    activation = activationGRU, kernel_initializer = kernelInitGRU,\n",
        "                                        return_sequences = TRUE,initial_state = hiddenStatesEncoderGRU)(decoderSeqFeat)))\n",
        "\n",
        "encoderDecoderModel = tf.keras.Model(\n",
        "    inputs      =   allSeqFeatLayerInputs,\n",
        "    outputs     =   decoderOutputs)\n",
        "\n",
        "if (useSkipGramEmbeddings):\n",
        "    encoderDecoderModel.get_layer(name = \"CustomerID_embed\").set_weights(list(skipGramEmbeddingMatrix)).trainable = False\n",
        "\n",
        "encoderDecoderModel.compile(optimizer = optimizerVal,\n",
        "                            loss      = lossVal)\n",
        "\n",
        "if (!useTrainGen & sampleFracCust == 1 & sampleFracItem == 1):\n",
        "    # rm = delete objects from the memory\n",
        "    del X_DT, customerFeaturesDT_all, subcategoryNumberOfOrdersSparseMatrix_all, customerOrderTimeDT\n",
        "    # gc() 好像python會自動清除垃圾 ref:https://www.tutorialspoint.com/How-does-garbage-collection-work-in-Python\n",
        "\n",
        "if (continueTrain):\n",
        "    encoderDecoderModel.load_weights(path + \"encoderDecoderModel_full_current_\" + dataset + \".hdf5\")\n",
        "\n",
        "# rowSums meaning ref:https://www.geeksforgeeks.org/compute-the-sum-of-rows-of-a-matrix-or-array-in-r-programming-rowsums-function/\n",
        "totalTrueVec = np.sum(Y_valid_Array[:,:, 1], axis=1)\n",
        "finalRmseTotalVec = 0\n",
        "\n",
        "if (use2Valid):\n",
        "    totalTrueVec_2 = np.sum(Y_valid_Array_2[:,:, 1], axis=1)\n",
        "    finalRmseTotalVec_2 = 0\n",
        "\n",
        "if (useTrainGen):\n",
        "    trainHistory = encoderDecoderModel.fit_generator(\n",
        "            generator           =   trainGeneratorSeqCustom(X_DT, batch_size = batchSizeVal), \n",
        "            steps_per_epoch     =   1,\n",
        "            epochs              =   nEpochVal,\n",
        "            max_queue_size      =   1,\n",
        "            validation_data     =   list(list(X_valid_Array), Y_valid_Array[, , 1, drop = FALSE]),\n",
        "            verbose             =   0,\n",
        "            callbacks           =   list(callback_early_stopping(monitor = \"val_loss\", patience = earlyStoppingPatienceVal),\n",
        "                                         callback_model_checkpoint(paste0(path, \"best_model_\", tryOutIt, \"_\", runTimestamp, \".h5\"), \n",
        "                                         save_best_only = TRUE, save_weights_only = TRUE)))\n",
        "\n",
        "    validLossVec = trainHistory[metrics][val_loss]\n",
        "\n",
        "else:\n",
        "    if (repeatSampleFit):\n",
        "        validLossVec = 0\n",
        "        for epochVal in range(nEpochVal):\n",
        "            if (epochVal > 0):\n",
        "                if (sampleFracCust < 1 | sampleFracItem < 1):\n",
        "                    # clvGenTrainArrays.r裡面的裡面的function\n",
        "                    trainGenSourceFun(passToGlobEnv = TRUE)\n",
        "\n",
        "            trainHistory = encoderDecoderModel.fit(\n",
        "                    x               =   list(X_train_Array),\n",
        "                    y               =   Y_train_Array[, , 1, drop = FALSE],\n",
        "                    epochs          =   1,\n",
        "                    batch_size      =   batchSizeVal,\n",
        "                    validation_data =   list(list(X_valid_Array), Y_valid_Array[, , 1, drop = FALSE]),\n",
        "                    verbose         =   0,\n",
        "                    callbacks       =   list(callback_early_stopping(monitor = \"val_loss\", patience = earlyStoppingPatienceVal),\n",
        "                                             callback_model_checkpoint(paste0(path, \"best_model_\", tryOutIt, \"_\", runTimestamp, \".h5\"), \n",
        "                                             save_best_only = TRUE, save_weights_only = TRUE))\n",
        "                )\n",
        "            \n",
        "            validLossVec = [validLossVec, trainHistory[metrics][val_loss]]\n",
        "            \n",
        "            y_valid_pred_Array = encoderDecoderModel.predict(X_valid_Array)\n",
        "            totalPredVec = np.sum(y_valid_pred_Array, axis=1)\n",
        "            finalRmseTotal = np.sqrt(mean((totalPredVec - totalTrueVec)^2))\n",
        "            finalRmseTotalVec = [finalRmseTotalVec, finalRmseTotal]\n",
        "            \n",
        "            if (use2Valid):\n",
        "                y_valid_pred_Array_2 = encoderDecoderModel.predict(X_valid_Array_2)\n",
        "                totalPredVec_2 = np.sum(y_valid_pred_Array_2, axis=1)\n",
        "                finalRmseTotal_2 = np.sqrt(mean((totalPredVec_2 - totalTrueVec_2)^2))\n",
        "                finalRmseTotalVec_2 = [finalRmseTotalVec_2, finalRmseTotal_2]\n",
        "            \n",
        "            if ((lossVal == \"mean_squared_error\") & (epochVal == 1 | min(finalRmseTotalVec) == finalRmseTotal)):\n",
        "                encoderDecoderModel.save_weights(filepath = path+\"encoderDecoderModel_currentbest_\"+dataset+\"_\"+tryOutIt+\"_\"+runTimestamp+\".h5\",\n",
        "                                                 overwrite = TRUE)\n",
        "\n",
        "            if ((np.argmin(validLossVec) < len(validLossVec) - earlyStoppingPatienceVal) | \n",
        "                (epochVal > stopIfErrorMoreThanPrevFactorDelay & \n",
        "                    validLossVec[len(validLossVec)] >= min(validLossVec[-len(validLossVec)]) * stopIfErrorMoreThanPrevFactor)):\n",
        "                break\n",
        "            \n",
        "    else:\n",
        "        trainHistory = encoderDecoderModel.fit(   x               = list(X_train_Array),\n",
        "                                                  y               = Y_train_Array[, , 1, drop = FALSE],\n",
        "                                                  epochs          = nEpochVal,\n",
        "                                                  batch_size      = batchSizeVal,\n",
        "                                                  validation_data = list(list(X_valid_Array), Y_valid_Array[, , 1, drop = FALSE]),\n",
        "                                                  verbose         = 0,\n",
        "                                                  callbacks       = list(callback_early_stopping(monitor = \"val_loss\", patience = earlyStoppingPatienceVal),\n",
        "                                                                        callback_model_checkpoint(paste0(path, \"best_model_\", tryOutIt, \"_\", runTimestamp, \".h5\"), \n",
        "                                                                        save_best_only = TRUE, save_weights_only = TRUE)))\n",
        "            \n",
        "        validLossVec = trainHistory[metrics][val_loss]\n",
        "    \n",
        "\n",
        "encoderDecoderModel.save_weights(filepath = path+\"encoderDecoderModel_full_\"+dataset+\"_\"+tryOutIt+\"_\"+runTimestamp+\".h5\",\n",
        "                                                 overwrite = TRUE)\n",
        "\n",
        "try:\n",
        "    encoderDecoderModel.load_weights(path + \"encoderDecoderModel_currentbest_\"+dataset+\"_\"+tryOutIt+\"_\"+runTimestamp+\".h5\")\n",
        "\n",
        "y_valid_pred_Array = encoderDecoderModel.predict(X_valid_Array)\n",
        "preds_valid_DT = temp_subslice_X_valid_wide_DT.loc[:, 'iterateVarVec']\n",
        "\n",
        "# ref:https://stackoverflow.com/questions/39050539/how-to-add-multiple-columns-to-pandas-dataframe-in-one-assignment\n",
        "preds_valid_DT = pd.concat([preds_valid_DT,pd.DataFrame([[y_valid_pred_Array.reshape(y_valid_pred_Array.shape[0], y_valid_pred_Array.shape[1])]],columns=[\"pred_\"+i for i in range(y_valid_pred_Array.shape[1]) ])],axis=1)\n",
        "true_valid_DT = temp_subslice_X_valid_wide_DT.loc[:, 'iterateVarVec']\n",
        "true_valid_DT = pd.concat([true_valid_DT,pd.DataFrame([[Y_valid_Array[:,:,1].reshape(Y_valid_Array[:,:,1].shape[0],Y_valid_Array[:,:,1].shape[1])]],columns=[\"true_\"+i for i in range(Y_valid_Array.shape[1]) ])],axis=1)\n",
        "\n",
        "if (use2Valid):\n",
        "    y_valid_pred_Array_2 = encoderDecoderModel.predict(X_valid_Array_2)\n",
        "    preds_valid_DT_2 = temp_subslice_X_valid_wide_DT_2.loc[:,'iterateVarVec']\n",
        "    preds_valid_DT_2 = pd.concat([preds_valid_DT_2,pd.DataFrame([[y_valid_pred_Array_2.reshape(y_valid_pred_Array_2.shape[0], y_valid_pred_Array_2.shape[1])]],columns=[\"pred_\"+i for i in range(y_valid_pred_Array_2.shape[1]) ])],axis=1)\n",
        "    true_valid_DT = temp_subslice_X_valid_wide_DT_2.loc[:, 'iterateVarVec']\n",
        "    true_valid_DT = pd.concat([true_valid_DT_2,pd.DataFrame([[Y_valid_Array_2[:,:,1].reshape(Y_valid_Array_2[:,:,1].shape[0],Y_valid_Array_2[:,:,1].shape[1])]],columns=[\"true_\"+i for i in range(Y_valid_Array_2.shape[1]) ])],axis=1)\n",
        "\n",
        "try:\n",
        "    preds_valid_DT.to_csv(path+''.join(\"predsEncDec_valid_DT_\", dataset, \"_\", tryOutIt, \".csv\"))\n",
        "    true_valid_DT.to_csv(''.join(path, \"trueEncDec_valid_DT_\", dataset, \"_\", tryOutIt, \".csv\"))\n",
        "\n",
        "if (use2Valid):\n",
        "    try:\n",
        "        preds_valid_DT_2.to_csv(path+''.join(\"predsEncDec_valid_DT_2_\", dataset, \"_\", tryOutIt, \".csv\"))\n",
        "        true_valid_DT_2.to_csv(''.join(path, \"trueEncDec_valid_DT_2_\", dataset, \"_\", tryOutIt, \".csv\"))\n",
        "\n",
        "\n",
        "\n",
        "finalRmseWeekwise = np.sqrt(mean((y_valid_pred_Array - Y_valid_Array[, , 1])**2))\n",
        "\n",
        "finalMseWeekwise = mean((y_valid_pred_Array - Y_valid_Array[, , 1])**2)\n",
        "\n",
        "totalTrueVec = np.sum(Y_valid_Array[:,:, 1], axis=1)\n",
        "\n",
        "totalPredVec = np.sum(y_valid_pred_Array, axis=1) \n",
        "\n",
        "finalRmseTotal = np.sqrt(mean((totalPredVec - totalTrueVec)**2))\n",
        "\n",
        "if (use2Valid):\n",
        "    min(finalRmseTotalVec_2)\n",
        "\n",
        "finalMseTotal = np.mean((totalPredVec - totalTrueVec)**2)\n",
        "\n",
        "finalRmseTotal_2 = finalRmseTotal\n",
        "\n",
        "if (use2Valid):\n",
        "\n",
        "    totalTrueVec_2 = np.sum(Y_valid_Array_2[:,:, 1], axis=1)\n",
        "    totalPredVec_2 = np.sum(y_valid_pred_Array_2, axis=1)\n",
        "    finalRmseTotal_2 = np.sqrt(mean((totalPredVec_2 - totalTrueVec_2)**2))\n",
        "    finalMseTotal_2 = mean((totalPredVec_2 - totalTrueVec_2)**2)\n",
        "\n",
        "minValidLossVal = min(validLossVec)\n",
        "argminValidLossVal = np.argmin(validLossVec)\n",
        "\n",
        "# 原本是list我改用我改用dict存\n",
        "saveParamResultList = {\n",
        "    dataset                             = dataset,\n",
        "    runTimestamp                        = runTimestamp,\n",
        "    tryOutIt                            = tryOutIt,\n",
        "    continueTrain                       = continueTrain,\n",
        "    sampleFracCust                      = sampleFracCust,\n",
        "    sampleFracItem                      = sampleFracItem,\n",
        "    timeVarName                         = timeVarName,\n",
        "    minTime                             = minTime,\n",
        "    maxTime                             = maxTime,\n",
        "    minValidTime                        = minValidTime,\n",
        "    maxValidTime                        = maxValidTime,\n",
        "    minTestTime                         = minTestTime,\n",
        "    maxTestTime                         = maxTestTime,\n",
        "    seqInLength                         = seqInLength,\n",
        "    seqOutLength                        = seqOutLength,\n",
        "    usePeriodicTrainData                = usePeriodicTrainData,\n",
        "    useSkipGramEmbeddings               = useSkipGramEmbeddings,\n",
        "    minRmseTotal                        = min(finalRmseTotalVec),\n",
        "    minRmseTotal_2                      = min(finalRmseTotalVec_2),\n",
        "    finalRmseTotalVec                   = paste(as.character(finalRmseTotalVec), collapse = \" \"),\n",
        "    finalRmseTotalVec_2                 = paste(as.character(finalRmseTotalVec_2), collapse = \" \"),\n",
        "    convPaddingType                     = convPaddingType,\n",
        "    useSeparateConv                     = useSeparateConv,\n",
        "    nUnitsEncoder                       = nUnitsEncoder,\n",
        "    nUnitsEncoder_2                     = nUnitsEncoder_2,\n",
        "    nUnitsDecoder                       = nUnitsDecoder,\n",
        "    dropoutEncoder                      = dropoutEncoder,\n",
        "    recurrentDropoutEncoder             = recurrentDropoutEncoder,\n",
        "    dropoutDecoder                      = dropoutDecoder,\n",
        "    recurrentDropoutDecoder             = recurrentDropoutDecoder,\n",
        "    learnRateVal                        = learnRateVal,\n",
        "    nEpochVal                           = nEpochVal}\n",
        "\n",
        "try:\n",
        "    # 最後面感覺只是存資料資料 我先寫成存json檔\n",
        "    with open(\"saveParamResult.json\", \"w\") as outfile:\n",
        "        json.dump(saveParamResultList, outfile)"
      ]
    }
  ]
}